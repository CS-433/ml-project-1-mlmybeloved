{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ce48c91",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e36b7a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from implementations import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacf590b",
   "metadata": {},
   "source": [
    "## Checking the first sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b2c623a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sample: [ 1.38470e+02  5.16550e+01  9.78270e+01  2.79800e+01  9.10000e-01\n",
      "  1.24711e+02  2.66600e+00  3.06400e+00  4.19280e+01  1.97760e+02\n",
      "  1.58200e+00  1.39600e+00  2.00000e-01  3.26380e+01  1.01700e+00\n",
      "  3.81000e-01  5.16260e+01  2.27300e+00 -2.41400e+00  1.68240e+01\n",
      " -2.77000e-01  2.58733e+02  2.00000e+00  6.74350e+01  2.15000e+00\n",
      "  4.44000e-01  4.60620e+01  1.24000e+00 -2.47500e+00  1.13497e+02]\n",
      "First sample label: 1\n"
     ]
    }
   ],
   "source": [
    "# Long running\n",
    "y, x = load_data(train=True)\n",
    "print(f\"First sample: {x[0,:]}\")\n",
    "print(f\"First sample label: {y[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c8ab06",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2974608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      " [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [5]]\n",
      "\n",
      "X normalized:\n",
      " [[-1.41421356]\n",
      " [-0.70710678]\n",
      " [ 0.        ]\n",
      " [ 0.70710678]\n",
      " [ 1.41421356]]\n",
      "\n",
      "X with bias:\n",
      " [[ 1.         -1.41421356]\n",
      " [ 1.         -0.70710678]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.70710678]\n",
      " [ 1.          1.41421356]]\n"
     ]
    }
   ],
   "source": [
    "# Define some test data\n",
    "testing_y = np.array([1,1,2,2,4])\n",
    "testing_x = np.array([[1],[2],[3],[4],[5]])\n",
    "testing_w = np.array([-0.1, 0.7])\n",
    "\n",
    "print(f\"X:\\n {testing_x}\\n\")\n",
    "testing_sx, testing_mean_x, testing_std_x = standardize(testing_x) # Standardization\n",
    "print(f\"X normalized:\\n {testing_sx}\\n\") \n",
    "testing_tx = add_x_bias(testing_sx) # Adding bias column to X\n",
    "print(f\"X with bias:\\n {testing_tx}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba53571c",
   "metadata": {},
   "source": [
    "### MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db60e35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE TEST\n",
      "Got:4.714070708874368\n",
      "Expected:4.71\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE TEST\")\n",
    "print(f\"Got:{compute_mse(testing_y, testing_tx, testing_w)}\")\n",
    "print(\"Expected:\" + str(4.71))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6857fad",
   "metadata": {},
   "source": [
    "### MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bb8337a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE TEST\n",
      "Got:2.1\n",
      "Expected:2.1\n"
     ]
    }
   ],
   "source": [
    "print(\"MAE TEST\")\n",
    "print(f\"Got:{compute_mae(testing_y, testing_tx, testing_w)}\")\n",
    "print(\"Expected:\" + str(2.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5d79bf",
   "metadata": {},
   "source": [
    "### MSE Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e397eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Gradient TEST\n",
      "Got:[-2.1        -0.28994949]\n",
      "Expected:[-2.1, -0.29]\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE Gradient TEST\")\n",
    "print(f\"Got:{compute_mse_gradient(testing_y, testing_tx, testing_w)}\")\n",
    "print(\"Expected:\" + str([-2.1, -0.29]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3da1cbe",
   "metadata": {},
   "source": [
    "### MAE Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b45a485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE Gradient TEST\n",
      "Got:[-1.  0.]\n",
      "Expected:[-1, 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"MAE Gradient TEST\")\n",
    "print(f\"Got:{compute_mae_gradient(testing_y, testing_tx, testing_w)}\")\n",
    "print(\"Expected:\" + str([-1, -0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd8eeb2",
   "metadata": {},
   "source": [
    "## Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58af40ee",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b9e2f596",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr, x_tr = load_data(train=True) # Load train data\n",
    "y_te, x_te = load_data(train=False) # Load test data\n",
    "x_tr = replace_min_999_by_col_mean(x_tr) # Handle invalid values\n",
    "x_te = replace_min_999_by_col_mean(x_te)\n",
    "\n",
    "x_tr, mean_x_tr, std_x_tr = standardize(x_tr) # Standardize x\n",
    "x_te, mean_x_te, std_x_te = standardize(x_te)\n",
    "\n",
    "tx_tr = build_poly(x_tr, 2) # build polynomial expansion (with bias)\n",
    "tx_te = build_poly(x_te, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd04e81f",
   "metadata": {},
   "source": [
    "### Linear regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3baccc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : Training loss: 0.28653501985302926 Test loss: 0.8568301408047532\n",
      "Epoch 1 : Training loss: 0.2646759091502792 Test loss: 0.787958207087495\n",
      "Epoch 2 : Training loss: 0.25188103877212425 Test loss: 0.7436585475593934\n",
      "Epoch 3 : Training loss: 0.24289694588171118 Test loss: 0.710779876106066\n",
      "Epoch 4 : Training loss: 0.236007111749431 Test loss: 0.6845113922487513\n",
      "Epoch 5 : Training loss: 0.23047164244235122 Test loss: 0.6626982526003737\n",
      "Epoch 6 : Training loss: 0.22589730737860683 Test loss: 0.6441895309927617\n",
      "Epoch 7 : Training loss: 0.22204050536862108 Test loss: 0.6282748733801345\n",
      "Epoch 8 : Training loss: 0.21873530797490556 Test loss: 0.6144673396874024\n",
      "Epoch 9 : Training loss: 0.21586259032879546 Test loss: 0.6024087586331878\n",
      "Epoch 10 : Training loss: 0.21333429913990562 Test loss: 0.5918231649854439\n",
      "Epoch 11 : Training loss: 0.2110841639748312 Test loss: 0.5824912315285625\n",
      "Epoch 12 : Training loss: 0.20906164226856097 Test loss: 0.5742348644669887\n",
      "Epoch 13 : Training loss: 0.20722774275712336 Test loss: 0.5669072007931792\n",
      "Epoch 14 : Training loss: 0.20555205626470158 Test loss: 0.5603857200006764\n",
      "Epoch 15 : Training loss: 0.20401060945039987 Test loss: 0.554567280023304\n",
      "Epoch 16 : Training loss: 0.20258429730092198 Test loss: 0.5493644131678435\n",
      "Epoch 17 : Training loss: 0.20125772944067433 Test loss: 0.544702487566842\n",
      "Epoch 18 : Training loss: 0.20001837519140547 Test loss: 0.5405174867094074\n",
      "Epoch 19 : Training loss: 0.19885592573013308 Test loss: 0.5367542442764328\n",
      "Epoch 20 : Training loss: 0.1977618148663519 Test loss: 0.5333650227767367\n",
      "Epoch 21 : Training loss: 0.19672885631178672 Test loss: 0.5303083569737675\n",
      "Epoch 22 : Training loss: 0.19575096696243294 Test loss: 0.5275481045356526\n",
      "Epoch 23 : Training loss: 0.19482295405209607 Test loss: 0.5250526609969193\n",
      "Epoch 24 : Training loss: 0.1939403500301568 Test loss: 0.5227943064390703\n",
      "Epoch 25 : Training loss: 0.19309928333701065 Test loss: 0.5207486587459353\n",
      "Epoch 26 : Training loss: 0.192296376374674 Test loss: 0.5188942137799272\n",
      "Epoch 27 : Training loss: 0.19152866423592518 Test loss: 0.5172119569434149\n",
      "Epoch 28 : Training loss: 0.19079352940441907 Test loss: 0.5156850337247926\n",
      "Epoch 29 : Training loss: 0.1900886488429545 Test loss: 0.5142984692468724\n",
      "Epoch 30 : Training loss: 0.18941195077095038 Test loss: 0.5130389287212193\n",
      "Epoch 31 : Training loss: 0.18876157908369515 Test loss: 0.5118945121978331\n",
      "Epoch 32 : Training loss: 0.18813586384867328 Test loss: 0.5108545781805711\n",
      "Epoch 33 : Training loss: 0.18753329667397453 Test loss: 0.5099095916249785\n",
      "Epoch 34 : Training loss: 0.18695251001347912 Test loss: 0.5090509925989306\n",
      "Epoch 35 : Training loss: 0.18639225967702636 Test loss: 0.5082710825068789\n",
      "Epoch 36 : Training loss: 0.18585140996843644 Test loss: 0.5075629252854681\n",
      "Epoch 37 : Training loss: 0.18532892099263792 Test loss: 0.5069202613947256\n",
      "Epoch 38 : Training loss: 0.1848238377644571 Test loss: 0.5063374327727563\n",
      "Epoch 39 : Training loss: 0.18433528082257938 Test loss: 0.5058093172068083\n",
      "Epoch 40 : Training loss: 0.18386243810776318 Test loss: 0.5053312708106498\n",
      "Epoch 41 : Training loss: 0.18340455790823515 Test loss: 0.504899077496188\n",
      "Epoch 42 : Training loss: 0.18296094271006766 Test loss: 0.5045089044930793\n",
      "Epoch 43 : Training loss: 0.1825309438182565 Test loss: 0.504157263109394\n",
      "Epoch 44 : Training loss: 0.18211395663674165 Test loss: 0.5038409740437118\n",
      "Epoch 45 : Training loss: 0.181709416513895 Test loss: 0.5035571366580907\n",
      "Epoch 46 : Training loss: 0.18131679507492876 Test loss: 0.5033031017051467\n",
      "Epoch 47 : Training loss: 0.18093559697495437 Test loss: 0.5030764470735516\n",
      "Epoch 48 : Training loss: 0.18056535701655232 Test loss: 0.5028749561766286\n",
      "Epoch 49 : Training loss: 0.18020563758412872 Test loss: 0.5026965986600976\n",
      "Epoch 50 : Training loss: 0.17985602635435283 Test loss: 0.5025395131488537\n",
      "Epoch 51 : Training loss: 0.17951613424784235 Test loss: 0.5024019917900784\n",
      "Epoch 52 : Training loss: 0.17918559359220823 Test loss: 0.5022824663820048\n",
      "Epoch 53 : Training loss: 0.17886405647073475 Test loss: 0.5021794959051201\n",
      "Epoch 54 : Training loss: 0.1785511932345019 Test loss: 0.5020917552961397\n",
      "Epoch 55 : Training loss: 0.17824669115875016 Test loss: 0.5020180253253672\n",
      "Epoch 56 : Training loss: 0.17795025322684185 Test loss: 0.5019571834555074\n",
      "Epoch 57 : Training loss: 0.1776615970273493 Test loss: 0.5019081955750719\n",
      "Epoch 58 : Training loss: 0.17738045375166306 Test loss: 0.5018701085125398\n",
      "Epoch 59 : Training loss: 0.17710656728111643 Test loss: 0.5018420432487176\n",
      "Epoch 60 : Training loss: 0.1768396933539949 Test loss: 0.501823188754519\n",
      "Epoch 61 : Training loss: 0.17657959880398508 Test loss: 0.5018127963899073\n",
      "Epoch 62 : Training loss: 0.17632606086264407 Test loss: 0.5018101748071228\n",
      "Epoch 63 : Training loss: 0.1760788665193537 Test loss: 0.5018146853077905\n",
      "Epoch 64 : Training loss: 0.17583781193299342 Test loss: 0.5018257376091438\n",
      "Epoch 65 : Training loss: 0.1756027018902347 Test loss: 0.5018427859795291\n",
      "Epoch 66 : Training loss: 0.1753733493059402 Test loss: 0.5018653257077041\n",
      "Epoch 67 : Training loss: 0.17514957476165935 Test loss: 0.5018928898742301\n",
      "Epoch 68 : Training loss: 0.17493120607865456 Test loss: 0.5019250463966292\n",
      "Epoch 69 : Training loss: 0.17471807792228122 Test loss: 0.5019613953229158\n",
      "Epoch 70 : Training loss: 0.17451003143488694 Test loss: 0.5020015663507292\n",
      "Epoch 71 : Training loss: 0.17430691389469155 Test loss: 0.502045216551609\n",
      "Epoch 72 : Training loss: 0.17410857839837693 Test loss: 0.502092028282001\n",
      "Epoch 73 : Training loss: 0.1739148835653473 Test loss: 0.5021417072643961\n",
      "Epoch 74 : Training loss: 0.1737256932618251 Test loss: 0.5021939808236364\n",
      "Epoch 75 : Training loss: 0.17354087634313212 Test loss: 0.5022485962648569\n",
      "Epoch 76 : Training loss: 0.1733603064126653 Test loss: 0.5023053193808312\n",
      "Epoch 77 : Training loss: 0.17318386159622162 Test loss: 0.5023639330776228\n",
      "Epoch 78 : Training loss: 0.17301142433045352 Test loss: 0.5024242361085067\n",
      "Epoch 79 : Training loss: 0.17284288116435126 Test loss: 0.5024860419070217\n",
      "Epoch 80 : Training loss: 0.17267812257274898 Test loss: 0.5025491775108719\n",
      "Epoch 81 : Training loss: 0.1725170427809441 Test loss: 0.5026134825691244\n",
      "Epoch 82 : Training loss: 0.17235953959960115 Test loss: 0.5026788084258406\n",
      "Epoch 83 : Training loss: 0.17220551426918151 Test loss: 0.5027450172738802\n",
      "Epoch 84 : Training loss: 0.1720548713132124 Test loss: 0.502811981373166\n",
      "Epoch 85 : Training loss: 0.17190751839976057 Test loss: 0.5028795823281993\n",
      "Epoch 86 : Training loss: 0.1717633662105362 Test loss: 0.502947710420063\n",
      "Epoch 87 : Training loss: 0.171622328317096 Test loss: 0.5030162639885596\n",
      "Epoch 88 : Training loss: 0.17148432106366074 Test loss: 0.5030851488604993\n",
      "Epoch 89 : Training loss: 0.17134926345610138 Test loss: 0.5031542778204926\n",
      "Epoch 90 : Training loss: 0.17121707705668388 Test loss: 0.5032235701209079\n",
      "Epoch 91 : Training loss: 0.17108768588419457 Test loss: 0.5032929510279281\n",
      "Epoch 92 : Training loss: 0.17096101631909882 Test loss: 0.5033623514009011\n",
      "Epoch 93 : Training loss: 0.17083699701341154 Test loss: 0.5034317073024049\n",
      "Epoch 94 : Training loss: 0.17071555880498318 Test loss: 0.503500959636662\n",
      "Epoch 95 : Training loss: 0.17059663463592784 Test loss: 0.5035700538141252\n",
      "Epoch 96 : Training loss: 0.17048015947493844 Test loss: 0.5036389394402434\n",
      "Epoch 97 : Training loss: 0.17036607024325545 Test loss: 0.5037075700265613\n"
     ]
    }
   ],
   "source": [
    "# We run GD step times per epoch, for epochs epochs (same as running GD for epochs*step just lets us print intermediate results)\n",
    "w, epochs, step, gamma = np.zeros(61), 100, 100, 1e-4\n",
    "loss_tr_GD = []\n",
    "loss_te_GD = []\n",
    "for i in range((int)(epochs)):\n",
    "    w, loss_tr = least_squares_GD(y_tr, tx_tr, w, step, gamma)\n",
    "    loss_te = compute_mse(y_te, tx_te, w)\n",
    "    loss_tr_GD.append(loss_tr)\n",
    "    loss_te_GD.append(loss_te)\n",
    "    print(f\"Epoch {i} : Training loss: {loss_tr} Test loss: {loss_te}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10adec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(loss_tr_GD)), loss_tr_GD, c='red')\n",
    "plt.plot(range(len(loss_te_GD)), loss_te_GD, c='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b9588c",
   "metadata": {},
   "source": [
    "### Linear regression using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "40630c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 61)\n",
      "(250000,)\n",
      "Epoch 0 : Training loss: 0.3325976914081756 Test loss: 0.976277911177939\n",
      "Epoch 1 : Training loss: 0.3238842786537892 Test loss: 0.9557873361616362\n",
      "Epoch 2 : Training loss: 0.3093048577740203 Test loss: 0.9260135398804302\n",
      "Epoch 3 : Training loss: 0.30277042163268175 Test loss: 0.8993444583511248\n",
      "Epoch 4 : Training loss: 0.2972807172571732 Test loss: 0.8773530529238618\n",
      "Epoch 5 : Training loss: 0.2969855940673768 Test loss: 0.87961412482193\n",
      "Epoch 6 : Training loss: 0.29427432382864666 Test loss: 0.8700675569943024\n",
      "Epoch 7 : Training loss: 0.29370545598339304 Test loss: 0.8582892691605222\n",
      "Epoch 8 : Training loss: 0.29282889551435326 Test loss: 0.8381590945025345\n",
      "Epoch 9 : Training loss: 0.2924761487162052 Test loss: 0.8362876407143413\n",
      "Epoch 10 : Training loss: 0.2923654915277618 Test loss: 0.8159807116154717\n",
      "Epoch 11 : Training loss: 0.2878650679929162 Test loss: 0.8335701158078369\n",
      "Epoch 12 : Training loss: 0.28338901580415593 Test loss: 0.8293527695870646\n",
      "Epoch 13 : Training loss: 0.28201733711761734 Test loss: 0.8168651616741829\n",
      "Epoch 14 : Training loss: 0.2795787128694499 Test loss: 0.821829619706905\n",
      "Epoch 15 : Training loss: 0.2784972570613116 Test loss: 0.8310484710488488\n",
      "Epoch 16 : Training loss: 0.27812582216276227 Test loss: 0.81464922803563\n",
      "Epoch 17 : Training loss: 0.27527298636254316 Test loss: 0.795041234899502\n",
      "Epoch 18 : Training loss: 0.273414253901677 Test loss: 0.7812287115919966\n",
      "Epoch 19 : Training loss: 0.2728038819157098 Test loss: 0.7790313117038882\n",
      "Epoch 20 : Training loss: 0.2716681129321949 Test loss: 0.7672093508599652\n",
      "Epoch 21 : Training loss: 0.2932486133458858 Test loss: 0.8225608626089982\n",
      "Epoch 22 : Training loss: 0.29185426995806774 Test loss: 0.8121122845333979\n",
      "Epoch 23 : Training loss: 0.2919406295092181 Test loss: 0.794283778612492\n",
      "Epoch 24 : Training loss: 0.29047076468012456 Test loss: 0.7816643920523896\n",
      "Epoch 25 : Training loss: 0.2884622084567606 Test loss: 0.7851871384719803\n",
      "Epoch 26 : Training loss: 0.28842034720574994 Test loss: 0.7897252798963721\n",
      "Epoch 27 : Training loss: 0.2888792772830821 Test loss: 0.8187750635519015\n",
      "Epoch 28 : Training loss: 0.2871608286540248 Test loss: 0.8160393498327332\n",
      "Epoch 29 : Training loss: 0.3019051364119628 Test loss: 0.7681704227297439\n",
      "Epoch 30 : Training loss: 0.3073081963228029 Test loss: 0.7512152882030938\n",
      "Epoch 31 : Training loss: 0.2945032555669816 Test loss: 0.7609364860428588\n",
      "Epoch 32 : Training loss: 0.29743492643577146 Test loss: 0.7471399363441207\n",
      "Epoch 33 : Training loss: 0.2989936076829414 Test loss: 0.7414126585283011\n",
      "Epoch 34 : Training loss: 0.2992586778234327 Test loss: 0.7288727286775605\n",
      "Epoch 35 : Training loss: 0.2917129398019369 Test loss: 0.7241312572189947\n",
      "Epoch 36 : Training loss: 0.2905912813576696 Test loss: 0.7231358204877016\n",
      "Epoch 37 : Training loss: 0.29371285740075925 Test loss: 0.7062531573720313\n",
      "Epoch 38 : Training loss: 0.2942614382150875 Test loss: 0.705460962168441\n",
      "Epoch 39 : Training loss: 0.3395968931033548 Test loss: 0.6989277521650781\n",
      "Epoch 40 : Training loss: 0.32688809685752285 Test loss: 0.7091733292679515\n",
      "Epoch 41 : Training loss: 0.32149513685754166 Test loss: 0.7335927713141482\n",
      "Epoch 42 : Training loss: 0.32324435516868505 Test loss: 0.7296959732412445\n",
      "Epoch 43 : Training loss: 0.33344132863204207 Test loss: 0.7127647634353452\n",
      "Epoch 44 : Training loss: 0.3306789013323982 Test loss: 0.713184473430737\n",
      "Epoch 45 : Training loss: 0.3298395288404531 Test loss: 0.717609825616016\n",
      "Epoch 46 : Training loss: 0.33140571153087073 Test loss: 0.7068874531292086\n",
      "Epoch 47 : Training loss: 0.3303996899808685 Test loss: 0.7063952214774196\n",
      "Epoch 48 : Training loss: 0.3366331124322939 Test loss: 0.6807217650908728\n",
      "Epoch 49 : Training loss: 0.3395448608955225 Test loss: 0.675744481846061\n",
      "Epoch 50 : Training loss: 0.33051089452440086 Test loss: 0.6757074211932846\n",
      "Epoch 51 : Training loss: 0.34280363055403296 Test loss: 0.6724366600415981\n",
      "Epoch 52 : Training loss: 0.28381925610868736 Test loss: 0.7108128618131041\n",
      "Epoch 53 : Training loss: 0.28419237585029383 Test loss: 0.7102493325324875\n",
      "Epoch 54 : Training loss: 0.36234579100160696 Test loss: 0.9190000469489465\n",
      "Epoch 55 : Training loss: 0.3533732227889814 Test loss: 0.8916713729912316\n",
      "Epoch 56 : Training loss: 0.351915044492638 Test loss: 0.8867609377942339\n",
      "Epoch 57 : Training loss: 0.34509644547057117 Test loss: 0.8504183830724561\n",
      "Epoch 58 : Training loss: 0.339510655719981 Test loss: 0.8319914368161169\n",
      "Epoch 59 : Training loss: 0.33630766607826923 Test loss: 0.8153970411794872\n",
      "Epoch 60 : Training loss: 0.33623107943801306 Test loss: 0.8107754654784355\n",
      "Epoch 61 : Training loss: 0.33439471986277614 Test loss: 0.8119795660032312\n",
      "Epoch 62 : Training loss: 0.3320316232353752 Test loss: 0.8048884548886056\n",
      "Epoch 63 : Training loss: 0.7693582057218273 Test loss: 0.7027344393445555\n",
      "Epoch 64 : Training loss: 0.7042379125018543 Test loss: 0.6971272541275514\n",
      "Epoch 65 : Training loss: 0.6912206397562118 Test loss: 0.6970302777872692\n",
      "Epoch 66 : Training loss: 0.6941933433037513 Test loss: 0.6965800977775491\n",
      "Epoch 67 : Training loss: 0.6983654485532345 Test loss: 0.6981112960442353\n",
      "Epoch 68 : Training loss: 0.6538955730162165 Test loss: 0.6972492110012083\n",
      "Epoch 69 : Training loss: 0.6561142369111748 Test loss: 0.6943193862692991\n",
      "Epoch 70 : Training loss: 0.6147501324198084 Test loss: 0.7135624873232262\n",
      "Epoch 71 : Training loss: 0.6182952053432386 Test loss: 0.7125481045573246\n",
      "Epoch 72 : Training loss: 0.4897661398702322 Test loss: 0.7381988433725776\n",
      "Epoch 73 : Training loss: 0.45985159592286223 Test loss: 0.7407504957489499\n",
      "Epoch 74 : Training loss: 0.46046234064122776 Test loss: 0.7410656896507635\n",
      "Epoch 75 : Training loss: 0.4672670948698032 Test loss: 0.735837921416888\n",
      "Epoch 76 : Training loss: 0.4694409163406212 Test loss: 0.7337134668947929\n",
      "Epoch 77 : Training loss: 0.30347159294169457 Test loss: 0.764996391082382\n",
      "Epoch 78 : Training loss: 0.30386723436493696 Test loss: 0.7650956776448947\n",
      "Epoch 79 : Training loss: 0.3040913671130487 Test loss: 0.7694805272746731\n",
      "Epoch 80 : Training loss: 0.30433426310533496 Test loss: 0.7730072634281235\n",
      "Epoch 81 : Training loss: 0.3024518162869482 Test loss: 0.7645387255338543\n",
      "Epoch 82 : Training loss: 0.30238361926812973 Test loss: 0.7627276410967984\n",
      "Epoch 83 : Training loss: 0.30453549805370633 Test loss: 0.7605202254024673\n",
      "Epoch 84 : Training loss: 0.3044478252894242 Test loss: 0.7575584820197894\n",
      "Epoch 85 : Training loss: 0.30628259463151586 Test loss: 0.7442860431142232\n",
      "Epoch 86 : Training loss: 0.3020624756240936 Test loss: 0.7235263129248173\n",
      "Epoch 87 : Training loss: 0.28483066454955586 Test loss: 0.717422847276875\n",
      "Epoch 88 : Training loss: 0.284928429085108 Test loss: 0.7178458999475446\n",
      "Epoch 89 : Training loss: 0.2774160956835584 Test loss: 0.7233377944190174\n",
      "Epoch 90 : Training loss: 0.2767065100063486 Test loss: 0.7117252360795973\n",
      "Epoch 91 : Training loss: 0.27781345618738695 Test loss: 0.7001457410827684\n",
      "Epoch 92 : Training loss: 0.2782631752967327 Test loss: 0.7030977821768173\n",
      "Epoch 93 : Training loss: 0.2998539341037438 Test loss: 0.6369493988868828\n",
      "Epoch 94 : Training loss: 0.3041711625652316 Test loss: 0.6359675523803952\n",
      "Epoch 95 : Training loss: 0.3029172352306941 Test loss: 0.6416312345880103\n",
      "Epoch 96 : Training loss: 0.2948094827364733 Test loss: 0.6624649515573143\n",
      "Epoch 97 : Training loss: 0.2951275569862534 Test loss: 0.6631221836214723\n",
      "Epoch 98 : Training loss: 0.3296675076274243 Test loss: 0.7232001925838216\n",
      "Epoch 99 : Training loss: 0.3224254505773596 Test loss: 0.710567009271871\n"
     ]
    }
   ],
   "source": [
    "w, epochs, step, gamma = np.zeros(61), 100, 100, 1e-4\n",
    "for i in range((int)(epochs)):\n",
    "    w, loss_tr = least_squares_SGD(y_tr, tx_tr, w, step, gamma)\n",
    "    loss_te = compute_mse(y_te, tx_te, w)\n",
    "    print(f\"Epoch {i} : Training loss: {loss_tr} Test loss: {loss_te}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c498d12",
   "metadata": {},
   "source": [
    "### Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3729fcfa",
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [56]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m w, loss_tr \u001b[38;5;241m=\u001b[39m \u001b[43mleast_squares\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtx_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m loss_te \u001b[38;5;241m=\u001b[39m compute_mse(y_te, tx_te, w)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_tr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTest loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_te\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mw: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Desktop\\Games and shit\\MLMaster\\ml-project-1-mlmybeloved\\implementations.py:244\u001b[0m, in \u001b[0;36mleast_squares\u001b[1;34m(y, tx)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mleast_squares\u001b[39m(y, tx):\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;124;03m\"\"\"Computes the least squares solution.\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;124;03m        mse: scalar, mean squared error.\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 244\u001b[0m     w \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[38;5;129;43m@tx\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[38;5;129;43m@y\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    245\u001b[0m     mse \u001b[38;5;241m=\u001b[39m compute_mse(y, tx, w)\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m w, mse\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36msolve\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\numpy\\linalg\\linalg.py:393\u001b[0m, in \u001b[0;36msolve\u001b[1;34m(a, b)\u001b[0m\n\u001b[0;32m    391\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDD->D\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdd->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    392\u001b[0m extobj \u001b[38;5;241m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[1;32m--> 393\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mgufunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap(r\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\numpy\\linalg\\linalg.py:88\u001b[0m, in \u001b[0;36m_raise_linalgerror_singular\u001b[1;34m(err, flag)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_linalgerror_singular\u001b[39m(err, flag):\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSingular matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "w, loss_tr = least_squares(y_tr, tx_tr)\n",
    "loss_te = compute_mse(y_te, tx_te, w)\n",
    "print(f\"Training loss: {loss_tr}\\nTest loss: {loss_te}\\nw: {w}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9557525a",
   "metadata": {},
   "source": [
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "741e5c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.17758079539317778\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 0.1\n",
    "w, loss_tr = ridge_regression(y_tr, tx_tr, lambda_)\n",
    "print(f\"Training loss: {loss}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51d88db",
   "metadata": {},
   "source": [
    "### Logistic regression using gradient descent or SGD (y ∈ {0, 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038db552",
   "metadata": {},
   "source": [
    "### Regularized logistic regression using gradient descent or SGD (y ∈ {0, 1}, with regularization term λ∥w∥**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77d3e58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
