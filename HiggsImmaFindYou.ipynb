{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ce48c91",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e36b7a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from implementations import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacf590b",
   "metadata": {},
   "source": [
    "## Checking the first sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b2c623a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sample: [ 1.38470e+02  5.16550e+01  9.78270e+01  2.79800e+01  9.10000e-01\n",
      "  1.24711e+02  2.66600e+00  3.06400e+00  4.19280e+01  1.97760e+02\n",
      "  1.58200e+00  1.39600e+00  2.00000e-01  3.26380e+01  1.01700e+00\n",
      "  3.81000e-01  5.16260e+01  2.27300e+00 -2.41400e+00  1.68240e+01\n",
      " -2.77000e-01  2.58733e+02  2.00000e+00  6.74350e+01  2.15000e+00\n",
      "  4.44000e-01  4.60620e+01  1.24000e+00 -2.47500e+00  1.13497e+02]\n",
      "First sample label: 1\n"
     ]
    }
   ],
   "source": [
    "# Long running\n",
    "y, x = load_data(train=True)\n",
    "print(f\"First sample: {x[0,:]}\")\n",
    "print(f\"First sample label: {y[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c8ab06",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2974608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      " [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [5]]\n",
      "\n",
      "X normalized:\n",
      " [[-1.41421356]\n",
      " [-0.70710678]\n",
      " [ 0.        ]\n",
      " [ 0.70710678]\n",
      " [ 1.41421356]]\n",
      "\n",
      "X with bias:\n",
      " [[ 1.         -1.41421356]\n",
      " [ 1.         -0.70710678]\n",
      " [ 1.          0.        ]\n",
      " [ 1.          0.70710678]\n",
      " [ 1.          1.41421356]]\n"
     ]
    }
   ],
   "source": [
    "# Define some test data\n",
    "testing_y = np.array([1,1,2,2,4])\n",
    "testing_x = np.array([[1],[2],[3],[4],[5]])\n",
    "testing_w = np.array([-0.1, 0.7])\n",
    "\n",
    "print(f\"X:\\n {testing_x}\\n\")\n",
    "testing_sx, testing_mean_x, testing_std_x = standardize(testing_x) # Standardization\n",
    "print(f\"X normalized:\\n {testing_sx}\\n\") \n",
    "testing_tx = add_x_bias(testing_sx) # Adding bias column to X\n",
    "print(f\"X with bias:\\n {testing_tx}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba53571c",
   "metadata": {},
   "source": [
    "### MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db60e35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE TEST\n",
      "Got:4.714070708874368\n",
      "Expected:4.71\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE TEST\")\n",
    "print(f\"Got:{compute_mse(testing_y, testing_tx, testing_w)}\")\n",
    "print(\"Expected:\" + str(4.71))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6857fad",
   "metadata": {},
   "source": [
    "### MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bb8337a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE TEST\n",
      "Got:2.1\n",
      "Expected:2.1\n"
     ]
    }
   ],
   "source": [
    "print(\"MAE TEST\")\n",
    "print(f\"Got:{compute_mae(testing_y, testing_tx, testing_w)}\")\n",
    "print(\"Expected:\" + str(2.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5d79bf",
   "metadata": {},
   "source": [
    "### MSE Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e397eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Gradient TEST\n",
      "Got:[-2.1        -0.28994949]\n",
      "Expected:[-2.1, -0.29]\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE Gradient TEST\")\n",
    "print(f\"Got:{compute_mse_gradient(testing_y, testing_tx, testing_w)}\")\n",
    "print(\"Expected:\" + str([-2.1, -0.29]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3da1cbe",
   "metadata": {},
   "source": [
    "### MAE Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b45a485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE Gradient TEST\n",
      "Got:[-1.  0.]\n",
      "Expected:[-1, 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"MAE Gradient TEST\")\n",
    "print(f\"Got:{compute_mae_gradient(testing_y, testing_tx, testing_w)}\")\n",
    "print(\"Expected:\" + str([-1, -0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd8eeb2",
   "metadata": {},
   "source": [
    "## Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58af40ee",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "72df273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a single cell because it takes a long time and doesn't need to be ran everytime\n",
    "y, x = load_data(train=True) # Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b9e2f596",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, x_te, y_tr, y_te = split_data(x, y, 0.8, np.random.seed())\n",
    "x_tr = replace_min_999_by_col_mean(x_tr) # Handle invalid values\n",
    "x_te = replace_min_999_by_col_mean(x_te)\n",
    "\n",
    "x_tr, mean_x_tr, std_x_tr = standardize(x_tr) # Standardize x\n",
    "x_te, mean_x_te, std_x_te = standardize(x_te)\n",
    "\n",
    "tx_tr = build_poly(x_tr, 2) # build polynomial expansion (with bias)\n",
    "tx_te = build_poly(x_te, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd04e81f",
   "metadata": {},
   "source": [
    "### Linear regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3baccc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : Training loss: 0.28628528428182864 Test loss: 0.2875664373080839\n",
      "Epoch 1 : Training loss: 0.264478868204983 Test loss: 0.26577015863599407\n",
      "Epoch 2 : Training loss: 0.25171837833427824 Test loss: 0.2529138604712575\n",
      "Epoch 3 : Training loss: 0.2427619970239877 Test loss: 0.2438545016266975\n",
      "Epoch 4 : Training loss: 0.23589708255435932 Test loss: 0.236889987240503\n",
      "Epoch 5 : Training loss: 0.23038194471872248 Test loss: 0.23128304040867484\n",
      "Epoch 6 : Training loss: 0.2258229683994911 Test loss: 0.2266428823277936\n",
      "Epoch 7 : Training loss: 0.22197747502875614 Test loss: 0.22272741940846028\n",
      "Epoch 8 : Training loss: 0.21868075020996927 Test loss: 0.2193709779846713\n",
      "Epoch 9 : Training loss: 0.21581468041994376 Test loss: 0.21645385158570488\n",
      "Epoch 10 : Training loss: 0.21329191441613968 Test loss: 0.21388704269755968\n",
      "Epoch 11 : Training loss: 0.2110466203198976 Test loss: 0.21160328279754365\n",
      "Epoch 12 : Training loss: 0.2090285088134412 Test loss: 0.20955112889984046\n",
      "Epoch 13 : Training loss: 0.20719872452150898 Test loss: 0.20769083942013983\n",
      "Epoch 14 : Training loss: 0.20552692545239687 Test loss: 0.20599140663584506\n",
      "Epoch 15 : Training loss: 0.20398916831561198 Test loss: 0.2044283908985831\n",
      "Epoch 16 : Training loss: 0.20256636013766888 Test loss: 0.20298232779806424\n",
      "Epoch 17 : Training loss: 0.20124311542791135 Test loss: 0.20163755034527694\n",
      "Epoch 18 : Training loss: 0.20000690692131734 Test loss: 0.20038131368061063\n",
      "Epoch 19 : Training loss: 0.19884743034371224 Test loss: 0.19920314112945053\n",
      "Epoch 20 : Training loss: 0.19775612606729567 Test loss: 0.19809433272395274\n",
      "Epoch 21 : Training loss: 0.19672581636021239 Test loss: 0.19704759338224057\n",
      "Epoch 22 : Training loss: 0.19575042824157118 Test loss: 0.1960567495693322\n",
      "Epoch 23 : Training loss: 0.1948247800780777 Test loss: 0.19511653169306703\n",
      "Epoch 24 : Training loss: 0.19394441592006062 Test loss: 0.1942224055981744\n",
      "Epoch 25 : Training loss: 0.19310547581705656 Test loss: 0.19337044095325195\n",
      "Epoch 26 : Training loss: 0.19230459343267384 Test loss: 0.19255720754345326\n",
      "Epoch 27 : Training loss: 0.19153881452064325 Test loss: 0.19177969282252788\n",
      "Epoch 28 : Training loss: 0.1908055314618083 Test loss: 0.1910352357846041\n",
      "Epoch 29 : Training loss: 0.1901024302624705 Test loss: 0.19032147346416067\n",
      "Epoch 30 : Training loss: 0.18942744729818672 Test loss: 0.1896362972884553\n",
      "Epoch 31 : Training loss: 0.18877873374033827 Test loss: 0.1889778171814337\n",
      "Epoch 32 : Training loss: 0.18815462608800185 Test loss: 0.18834433181757393\n",
      "Epoch 33 : Training loss: 0.18755362158995378 Test loss: 0.18773430379568767\n",
      "Epoch 34 : Training loss: 0.18697435761371778 Test loss: 0.187146338780707\n",
      "Epoch 35 : Training loss: 0.18641559422413878 Test loss: 0.18657916787077886\n",
      "Epoch 36 : Training loss: 0.18587619939030448 Test loss: 0.1860316326055933\n",
      "Epoch 37 : Training loss: 0.18535513635933845 Test loss: 0.18550267215291585\n",
      "Epoch 38 : Training loss: 0.18485145282789758 Test loss: 0.18499131230334864\n",
      "Epoch 39 : Training loss: 0.18436427161390465 Test loss: 0.18449665597542808\n",
      "Epoch 40 : Training loss: 0.183892782587147 Test loss: 0.18401787498943833\n",
      "Epoch 41 : Training loss: 0.1834362356615942 Test loss: 0.18355420291258737\n",
      "Epoch 42 : Training loss: 0.18299393468739542 Test loss: 0.18310492881328796\n",
      "Epoch 43 : Training loss: 0.18256523210859638 Test loss: 0.18266939179031874\n",
      "Epoch 44 : Training loss: 0.18214952427521436 Test loss: 0.18224697616519606\n",
      "Epoch 45 : Training loss: 0.18174624731663697 Test loss: 0.18183710724435942\n",
      "Epoch 46 : Training loss: 0.18135487349824264 Test loss: 0.1814392475726795\n",
      "Epoch 47 : Training loss: 0.1809749079954019 Test loss: 0.1810528936120257\n",
      "Epoch 48 : Training loss: 0.18060588602912372 Test loss: 0.18067757278872484\n",
      "Epoch 49 : Training loss: 0.1802473703159903 Test loss: 0.180312840862116\n",
      "Epoch 50 : Training loss: 0.17989894879200696 Test loss: 0.17995827957339186\n",
      "Epoch 51 : Training loss: 0.17956023257582868 Test loss: 0.17961349453976252\n",
      "Epoch 52 : Training loss: 0.17923085414173184 Test loss: 0.1792781133638983\n",
      "Epoch 53 : Training loss: 0.1789104656768349 Test loss: 0.1789517839327603\n",
      "Epoch 54 : Training loss: 0.1785987376005682 Test loss: 0.1786341728834438\n",
      "Epoch 55 : Training loss: 0.1782953572273628 Test loss: 0.17832496421665198\n",
      "Epoch 56 : Training loss: 0.1780000275560553 Test loss: 0.17802385804096205\n",
      "Epoch 57 : Training loss: 0.17771246617166322 Test loss: 0.17773056943322876\n",
      "Epoch 58 : Training loss: 0.17743240424702994 Test loss: 0.177444827402334\n",
      "Epoch 59 : Training loss: 0.17715958563342474 Test loss: 0.1771663739450978\n",
      "Epoch 60 : Training loss: 0.17689376603054346 Test loss: 0.17689496318454537\n",
      "Epoch 61 : Training loss: 0.17663471222753052 Test loss: 0.17663036058191964\n",
      "Epoch 62 : Training loss: 0.17638220140765784 Test loss: 0.17637234221485984\n",
      "Epoch 63 : Training loss: 0.17613602051017369 Test loss: 0.17612069411505787\n"
     ]
    }
   ],
   "source": [
    "# We run GD step times per epoch, for epochs epochs (same as running GD for epochs*step just lets us print intermediate results)\n",
    "w_GD, epochs, step, gamma = np.zeros(61), 100, 100, 1e-4\n",
    "loss_tr_GD = []\n",
    "loss_te_GD = []\n",
    "for i in range((int)(epochs)):\n",
    "    w_GD, loss_tr = least_squares_GD(y_tr, tx_tr, w_GD, step, gamma)\n",
    "    loss_te = compute_mse(y_te, tx_te, w_GD)\n",
    "    loss_tr_GD.append(loss_tr)\n",
    "    loss_te_GD.append(loss_te)\n",
    "    print(f\"Epoch {i} : Training loss: {loss_tr} Test loss: {loss_te}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c01a141",
   "metadata": {},
   "source": [
    "#### Plotting the resulting losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10adec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(loss_tr_GD)), loss_tr_GD, c='red')\n",
    "plt.plot(range(len(loss_te_GD)), loss_te_GD, c='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e8619a",
   "metadata": {},
   "source": [
    "#### Calculating the accuracy on the test set (with predictions = 0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3625fe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_cont = tx_te@w_GD\n",
    "y_hat = [1 if yi > 0.40 else 0 for yi in y_hat_cont]\n",
    "accuracy = 1-abs(y_te-y_hat).mean()\n",
    "print(f\"Accuracy for these w: {accuracy*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b9588c",
   "metadata": {},
   "source": [
    "### Linear regression using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "40630c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 61)\n",
      "(250000,)\n",
      "Epoch 0 : Training loss: 0.3325976914081756 Test loss: 0.976277911177939\n",
      "Epoch 1 : Training loss: 0.3238842786537892 Test loss: 0.9557873361616362\n",
      "Epoch 2 : Training loss: 0.3093048577740203 Test loss: 0.9260135398804302\n",
      "Epoch 3 : Training loss: 0.30277042163268175 Test loss: 0.8993444583511248\n",
      "Epoch 4 : Training loss: 0.2972807172571732 Test loss: 0.8773530529238618\n",
      "Epoch 5 : Training loss: 0.2969855940673768 Test loss: 0.87961412482193\n",
      "Epoch 6 : Training loss: 0.29427432382864666 Test loss: 0.8700675569943024\n",
      "Epoch 7 : Training loss: 0.29370545598339304 Test loss: 0.8582892691605222\n",
      "Epoch 8 : Training loss: 0.29282889551435326 Test loss: 0.8381590945025345\n",
      "Epoch 9 : Training loss: 0.2924761487162052 Test loss: 0.8362876407143413\n",
      "Epoch 10 : Training loss: 0.2923654915277618 Test loss: 0.8159807116154717\n",
      "Epoch 11 : Training loss: 0.2878650679929162 Test loss: 0.8335701158078369\n",
      "Epoch 12 : Training loss: 0.28338901580415593 Test loss: 0.8293527695870646\n",
      "Epoch 13 : Training loss: 0.28201733711761734 Test loss: 0.8168651616741829\n",
      "Epoch 14 : Training loss: 0.2795787128694499 Test loss: 0.821829619706905\n",
      "Epoch 15 : Training loss: 0.2784972570613116 Test loss: 0.8310484710488488\n",
      "Epoch 16 : Training loss: 0.27812582216276227 Test loss: 0.81464922803563\n",
      "Epoch 17 : Training loss: 0.27527298636254316 Test loss: 0.795041234899502\n",
      "Epoch 18 : Training loss: 0.273414253901677 Test loss: 0.7812287115919966\n",
      "Epoch 19 : Training loss: 0.2728038819157098 Test loss: 0.7790313117038882\n",
      "Epoch 20 : Training loss: 0.2716681129321949 Test loss: 0.7672093508599652\n",
      "Epoch 21 : Training loss: 0.2932486133458858 Test loss: 0.8225608626089982\n",
      "Epoch 22 : Training loss: 0.29185426995806774 Test loss: 0.8121122845333979\n",
      "Epoch 23 : Training loss: 0.2919406295092181 Test loss: 0.794283778612492\n",
      "Epoch 24 : Training loss: 0.29047076468012456 Test loss: 0.7816643920523896\n",
      "Epoch 25 : Training loss: 0.2884622084567606 Test loss: 0.7851871384719803\n",
      "Epoch 26 : Training loss: 0.28842034720574994 Test loss: 0.7897252798963721\n",
      "Epoch 27 : Training loss: 0.2888792772830821 Test loss: 0.8187750635519015\n",
      "Epoch 28 : Training loss: 0.2871608286540248 Test loss: 0.8160393498327332\n",
      "Epoch 29 : Training loss: 0.3019051364119628 Test loss: 0.7681704227297439\n",
      "Epoch 30 : Training loss: 0.3073081963228029 Test loss: 0.7512152882030938\n",
      "Epoch 31 : Training loss: 0.2945032555669816 Test loss: 0.7609364860428588\n",
      "Epoch 32 : Training loss: 0.29743492643577146 Test loss: 0.7471399363441207\n",
      "Epoch 33 : Training loss: 0.2989936076829414 Test loss: 0.7414126585283011\n",
      "Epoch 34 : Training loss: 0.2992586778234327 Test loss: 0.7288727286775605\n",
      "Epoch 35 : Training loss: 0.2917129398019369 Test loss: 0.7241312572189947\n",
      "Epoch 36 : Training loss: 0.2905912813576696 Test loss: 0.7231358204877016\n",
      "Epoch 37 : Training loss: 0.29371285740075925 Test loss: 0.7062531573720313\n",
      "Epoch 38 : Training loss: 0.2942614382150875 Test loss: 0.705460962168441\n",
      "Epoch 39 : Training loss: 0.3395968931033548 Test loss: 0.6989277521650781\n",
      "Epoch 40 : Training loss: 0.32688809685752285 Test loss: 0.7091733292679515\n",
      "Epoch 41 : Training loss: 0.32149513685754166 Test loss: 0.7335927713141482\n",
      "Epoch 42 : Training loss: 0.32324435516868505 Test loss: 0.7296959732412445\n",
      "Epoch 43 : Training loss: 0.33344132863204207 Test loss: 0.7127647634353452\n",
      "Epoch 44 : Training loss: 0.3306789013323982 Test loss: 0.713184473430737\n",
      "Epoch 45 : Training loss: 0.3298395288404531 Test loss: 0.717609825616016\n",
      "Epoch 46 : Training loss: 0.33140571153087073 Test loss: 0.7068874531292086\n",
      "Epoch 47 : Training loss: 0.3303996899808685 Test loss: 0.7063952214774196\n",
      "Epoch 48 : Training loss: 0.3366331124322939 Test loss: 0.6807217650908728\n",
      "Epoch 49 : Training loss: 0.3395448608955225 Test loss: 0.675744481846061\n",
      "Epoch 50 : Training loss: 0.33051089452440086 Test loss: 0.6757074211932846\n",
      "Epoch 51 : Training loss: 0.34280363055403296 Test loss: 0.6724366600415981\n",
      "Epoch 52 : Training loss: 0.28381925610868736 Test loss: 0.7108128618131041\n",
      "Epoch 53 : Training loss: 0.28419237585029383 Test loss: 0.7102493325324875\n",
      "Epoch 54 : Training loss: 0.36234579100160696 Test loss: 0.9190000469489465\n",
      "Epoch 55 : Training loss: 0.3533732227889814 Test loss: 0.8916713729912316\n",
      "Epoch 56 : Training loss: 0.351915044492638 Test loss: 0.8867609377942339\n",
      "Epoch 57 : Training loss: 0.34509644547057117 Test loss: 0.8504183830724561\n",
      "Epoch 58 : Training loss: 0.339510655719981 Test loss: 0.8319914368161169\n",
      "Epoch 59 : Training loss: 0.33630766607826923 Test loss: 0.8153970411794872\n",
      "Epoch 60 : Training loss: 0.33623107943801306 Test loss: 0.8107754654784355\n",
      "Epoch 61 : Training loss: 0.33439471986277614 Test loss: 0.8119795660032312\n",
      "Epoch 62 : Training loss: 0.3320316232353752 Test loss: 0.8048884548886056\n",
      "Epoch 63 : Training loss: 0.7693582057218273 Test loss: 0.7027344393445555\n",
      "Epoch 64 : Training loss: 0.7042379125018543 Test loss: 0.6971272541275514\n",
      "Epoch 65 : Training loss: 0.6912206397562118 Test loss: 0.6970302777872692\n",
      "Epoch 66 : Training loss: 0.6941933433037513 Test loss: 0.6965800977775491\n",
      "Epoch 67 : Training loss: 0.6983654485532345 Test loss: 0.6981112960442353\n",
      "Epoch 68 : Training loss: 0.6538955730162165 Test loss: 0.6972492110012083\n",
      "Epoch 69 : Training loss: 0.6561142369111748 Test loss: 0.6943193862692991\n",
      "Epoch 70 : Training loss: 0.6147501324198084 Test loss: 0.7135624873232262\n",
      "Epoch 71 : Training loss: 0.6182952053432386 Test loss: 0.7125481045573246\n",
      "Epoch 72 : Training loss: 0.4897661398702322 Test loss: 0.7381988433725776\n",
      "Epoch 73 : Training loss: 0.45985159592286223 Test loss: 0.7407504957489499\n",
      "Epoch 74 : Training loss: 0.46046234064122776 Test loss: 0.7410656896507635\n",
      "Epoch 75 : Training loss: 0.4672670948698032 Test loss: 0.735837921416888\n",
      "Epoch 76 : Training loss: 0.4694409163406212 Test loss: 0.7337134668947929\n",
      "Epoch 77 : Training loss: 0.30347159294169457 Test loss: 0.764996391082382\n",
      "Epoch 78 : Training loss: 0.30386723436493696 Test loss: 0.7650956776448947\n",
      "Epoch 79 : Training loss: 0.3040913671130487 Test loss: 0.7694805272746731\n",
      "Epoch 80 : Training loss: 0.30433426310533496 Test loss: 0.7730072634281235\n",
      "Epoch 81 : Training loss: 0.3024518162869482 Test loss: 0.7645387255338543\n",
      "Epoch 82 : Training loss: 0.30238361926812973 Test loss: 0.7627276410967984\n",
      "Epoch 83 : Training loss: 0.30453549805370633 Test loss: 0.7605202254024673\n",
      "Epoch 84 : Training loss: 0.3044478252894242 Test loss: 0.7575584820197894\n",
      "Epoch 85 : Training loss: 0.30628259463151586 Test loss: 0.7442860431142232\n",
      "Epoch 86 : Training loss: 0.3020624756240936 Test loss: 0.7235263129248173\n",
      "Epoch 87 : Training loss: 0.28483066454955586 Test loss: 0.717422847276875\n",
      "Epoch 88 : Training loss: 0.284928429085108 Test loss: 0.7178458999475446\n",
      "Epoch 89 : Training loss: 0.2774160956835584 Test loss: 0.7233377944190174\n",
      "Epoch 90 : Training loss: 0.2767065100063486 Test loss: 0.7117252360795973\n",
      "Epoch 91 : Training loss: 0.27781345618738695 Test loss: 0.7001457410827684\n",
      "Epoch 92 : Training loss: 0.2782631752967327 Test loss: 0.7030977821768173\n",
      "Epoch 93 : Training loss: 0.2998539341037438 Test loss: 0.6369493988868828\n",
      "Epoch 94 : Training loss: 0.3041711625652316 Test loss: 0.6359675523803952\n",
      "Epoch 95 : Training loss: 0.3029172352306941 Test loss: 0.6416312345880103\n",
      "Epoch 96 : Training loss: 0.2948094827364733 Test loss: 0.6624649515573143\n",
      "Epoch 97 : Training loss: 0.2951275569862534 Test loss: 0.6631221836214723\n",
      "Epoch 98 : Training loss: 0.3296675076274243 Test loss: 0.7232001925838216\n",
      "Epoch 99 : Training loss: 0.3224254505773596 Test loss: 0.710567009271871\n"
     ]
    }
   ],
   "source": [
    "w_SGD, epochs, step, gamma = np.zeros(61), 100, 100, 1e-4\n",
    "for i in range((int)(epochs)):\n",
    "    w_SGD, loss_tr = least_squares_SGD(y_tr, tx_tr, w_SGD, step, gamma)\n",
    "    loss_te = compute_mse(y_te, tx_te, w_SGD)\n",
    "    print(f\"Epoch {i} : Training loss: {loss_tr} Test loss: {loss_te}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c498d12",
   "metadata": {},
   "source": [
    "### Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3729fcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.15775399909190493\n",
      "Test loss: 1.0311299950200723\n",
      "w: [ 3.37137664e-01  6.52970377e-02 -1.22701137e-01 -1.52811873e-01\n",
      "  7.14286545e-02 -2.48003253e-02 -1.22351877e-02 -3.23113961e-02\n",
      "  1.31578040e-01 -5.80539364e-03 -2.24032327e+02 -9.88973360e-02\n",
      "  2.14275181e-02  3.53804357e-02  4.34080174e+01  5.00657117e-04\n",
      "  3.13543582e-04  4.26889017e+01  6.03112117e-05  6.10099853e-04\n",
      "  2.18336066e-03 -3.79707632e-04 -1.38501500e-02 -1.60504481e-02\n",
      "  4.89221318e-02 -3.53733241e-04 -6.31491563e-04  1.74254852e-02\n",
      "  7.00537917e-04 -7.62619593e-04  1.89889210e+02 -9.21111198e-03\n",
      "  1.82965859e-02  7.96031102e-03  1.47479101e-03  1.00063298e-02\n",
      "  2.30455444e-04 -3.84925914e-03 -2.91992065e-02  6.31294402e-04\n",
      " -5.83427643e-03  9.56267826e-03  2.67813328e-02  1.00913256e-02\n",
      " -6.23720001e-03 -1.66025009e-02 -1.53207500e-03 -9.60052673e-03\n",
      " -2.85748956e-02 -2.94577334e-05 -2.09764595e-03 -2.02235377e-04\n",
      " -1.09688513e-02 -2.03131101e-02 -5.79415916e-03  3.83367504e-02\n",
      " -1.11026240e-04 -3.90589551e-04  2.07797134e-02 -4.22502951e-04\n",
      "  1.18963991e-02]\n"
     ]
    }
   ],
   "source": [
    "w_LS, loss_tr = least_squares(y_tr, tx_tr)\n",
    "loss_te = compute_mse(y_te, tx_te, w_LS)\n",
    "print(f\"Training loss: {loss_tr}\\nTest loss: {loss_te}\\nw: {w_LS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9557525a",
   "metadata": {},
   "source": [
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "741e5c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.17758079539317778\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 0.1\n",
    "w_REG, loss_tr = ridge_regression(y_tr, tx_tr, lambda_)\n",
    "print(f\"Training loss: {loss}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51d88db",
   "metadata": {},
   "source": [
    "### Logistic regression using gradient descent or SGD (y ∈ {0, 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038db552",
   "metadata": {},
   "source": [
    "### Regularized logistic regression using gradient descent or SGD (y ∈ {0, 1}, with regularization term λ∥w∥**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77d3e58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
